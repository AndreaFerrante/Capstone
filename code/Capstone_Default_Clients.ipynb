{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a457ef",
   "metadata": {},
   "source": [
    "# Credit Default Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee6276",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009ba58",
   "metadata": {},
   "source": [
    "This Jupyter notebook increments a base line model performance (**i.e. Logistic Regression initial model**) over a credit default dataset. Therefore, this notebook is specifiacally designed for financial and mathematical analyst in the *credit risk assessment financial industry*.\n",
    "\n",
    "The problem that we are going to solve is a **binary classification problem**: is our new client going to default ?\n",
    "\n",
    "The whole notebook is divided accorsing to the following sections:\n",
    "\n",
    "1. **[Service factory](#service_factory)**: \n",
    "\n",
    "    This is a collection of functions that are multiple times used all over the notebook. The good thing about it is that *they are portable*: any coder that will be cut and pasting this notebook can re-adapt the to his/her needs.<br><br>\n",
    "\n",
    "2. **[Exploratory data analysis](#eda)**: \n",
    "\n",
    "Understanding the *business subject is important* as much as good modelling. Therefore, I code EDA first. In this part of the notebook we can \n",
    "<br>\n",
    "\n",
    "3. **[Model Selection](#model_selection)**: \n",
    "\n",
    "We have been comparing different mode\n",
    "<br>\n",
    "\n",
    "4. **[Conclusions](#conclusions)**:\n",
    "<br>\n",
    "\n",
    "5. **[Appendix](#appendix)**: \n",
    "\n",
    "In this notebook we have performed models *one after the other*. We would like to shocase a *lazy* but less customizable approach for the curious reader to explore !\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23166e05",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d8066",
   "metadata": {},
   "source": [
    "### Service Factory <a class=\"anchor\" id=\"service_factory\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create general contants and overall warning disabling together with general required packages installation.\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##################################\n",
    "RANDOM_STATE = 999\n",
    "MAIN_PATH    = os.getcwd()\n",
    "##################################\n",
    "\n",
    "## Uncomment the following line to install all the packages used in this notebook.\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib seaborn xlrd lazypredict\n",
    "# !{sys.executable} -m pip install networkx --force-reinstall --no-deps --upgrade --user\n",
    "# !{sys.executable} -m pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayes_opt():\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standar_scaler():\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc():\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_metrics():\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84877a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train_for_all():\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9cd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073deb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87f54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a0e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64dd896a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5e216",
   "metadata": {},
   "source": [
    "### Exploratory data analysis (EDA) <a class=\"anchor\" id=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae353e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67012286",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "print(f'Main path for this project is {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13eb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and perform EDA\n",
    "\n",
    "credit_data = pd.read_csv('./data/UCI_Credit_Card.csv')\n",
    "credit_data = credit_data.rename(columns = {'default.payment.next.month':'default'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we even begin, let's see the datatypes of our data and its types...\n",
    "\n",
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we even continue, let's spot possible missing data to then manage them...\n",
    "# ...in pandas is just one line of code !\n",
    "\n",
    "credit_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, we are speaking about this dataframe:\n",
    "\n",
    "credit_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f2509",
   "metadata": {},
   "source": [
    "There are no missing datapoints so we will be not performing any missing data management as shown [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/).\n",
    "\n",
    "*After* initial missing data and types exploration is performed, we can do some basic *Statistics and Correlations* as follows:\n",
    "\n",
    "1. we displayed the **basic statistics** of all the features using data.describe().\n",
    "2. we visualized the **correlation matrix** using a heatmap to understand the relationships between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's perform some correlations\n",
    "\n",
    "#print(credit_data.describe())\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(credit_data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa351a2",
   "metadata": {},
   "source": [
    "We are supposed to have a distribution of Numerical Features now by **plotting histograms for all the numerical features to visualize their distributions, just as follows.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Let's visualise the distribution of all the numerical features...\n",
    "\n",
    "numerical_features = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', \n",
    "                      'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "for feature in numerical_features:\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.histplot(credit_data[feature], kde=True)\n",
    "    plt.title(f'{feature} Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529089a",
   "metadata": {},
   "source": [
    "As we did for numerical features, we perform the same over categorical features with count plots just *using count plots for all the categorical features to visualize the count of each category in the features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e88b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Let's visualise the distribution of all the numerical features...\n",
    "\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.countplot(x=feature, data=credit_data)\n",
    "    plt.title(f'{feature} Count Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f23fd0",
   "metadata": {},
   "source": [
    "We know investigate over *Default Rates Across Different Categories*. <br>\n",
    "To do so, we do bar plots to **visualize the default rates across different categories** in the categorical features just as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98311c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Let's visualise the distribution of the default rates across different categories...\n",
    "\n",
    "for feature in categorical_features:\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.barplot(x=feature, y='default', data=credit_data)\n",
    "    plt.title(f'Default Rate by {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c201d",
   "metadata": {},
   "source": [
    "We can perform one last analysis now: **we plot a pair plot for a subset of variables to visualize the relationships between them and how they are affected by the target variable ('default').**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd122c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise the pair plot for a subset of variables to visualize relationships\n",
    "\n",
    "sns.pairplot(credit_data[['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'PAY_AMT1', 'default']], hue='default')\n",
    "plt.title('Pair Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279381c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09dcd5",
   "metadata": {},
   "source": [
    "We must now keep an eye on the nature of how much this **dataset is balanced (or not)***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e4336",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a825049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem we're are going to solve is imbalanced: let's see how much it is imbalanced\n",
    "\n",
    "credit_data['default'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ec60c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bb64d",
   "metadata": {},
   "source": [
    "We see that the **77.88% of our data is about good payers while the remaining is about \"defaulters\"**. <br>\n",
    "Since our dataset has 30,000 observations, if mean we have ca. 6636 bad payers.\n",
    "\n",
    "Our data is not that imbalanced as we could have been thinking at the benigging which means, under a business point of view, that the institution that has this dataset has lots of bad payers overall. <br>\n",
    "\n",
    "In order to be accurate in out predictions, we are going to leverage the following tecniques to balance this dataset:\n",
    "\n",
    "1. [SMOTE](https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/smote?view=azureml-api-2)\n",
    "2. [Oversampling](https://en.wikipedia.org/wiki/Oversampling)\n",
    "3. [Undersampling](https://en.wikipedia.org/wiki/Undersampling)\n",
    "\n",
    "As follows the code to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18709a27",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6866e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 1: Split the data into training and testing sets\n",
    "\n",
    "X = credit_data.drop('default', axis=1)\n",
    "y = credit_data['default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 2: Apply SMOTE\n",
    "\n",
    "smote            = SMOTE(random_state=RANDOM_STATE)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'SMOTE shape is {X_smote.shape}')\n",
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 3: apply random oversampling\n",
    "\n",
    "oversampler                  = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_oversampled, y_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Oversampled shape is {X_oversampled.shape}')\n",
    "y_oversampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 4: apply random undersampling\n",
    "\n",
    "undersampler                   = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_undersampled, y_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Undersample shape is {X_undersampled.shape}')\n",
    "y_undersampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b59212",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb70119",
   "metadata": {},
   "source": [
    "From what we have coded here above we can summarise:\n",
    "\n",
    ">- **SMOTE**: we applied the SMOTE technique to generate synthetic samples of the minority class in the training data, helping to balance the class distribution.\n",
    ">- **Random Oversampling**: we oversampled the minority class by randomly selecting samples with replacement, increasing the number of minority class samples in the training data.\n",
    ">- **Random Undersampling**: we undersampled the majority class by randomly removing samples, reducing the number of majority class samples in the training data to balance the class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952c83d",
   "metadata": {},
   "source": [
    "The *undersampled dataset* has a the best final ratio since it has \"just\" the bigger class (not defaulted) undersampled having still enough datapoints to do **inference** on (i.e 10,664 observations in total, equally split between defualt and not default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2a027",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final dataset will be then what follows\n",
    "\n",
    "y_undersampled       = pd.DataFrame(np.array(y_undersampled), columns=['default'])\n",
    "credit_data_balanced = pd.concat([X_undersampled, y_undersampled], axis=1)\n",
    "\n",
    "# This is going to be out balanced dataset...finally !\n",
    "credit_data_balanced.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b53263",
   "metadata": {},
   "source": [
    "We will be keeping on executing out code on the **credit_data_balanced** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f58d7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0ba47",
   "metadata": {},
   "source": [
    "### Model Selection <a class=\"anchor\" id=\"model_selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f9258",
   "metadata": {},
   "source": [
    "We will start with an hypotestis: out best model is **a base model centered on the Logistic Regression**.\n",
    "\n",
    "Any of the future model that will be performing better than a \"simple\" Logistic Regression will be acceppted like our **refrence model** for the resolution of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbac2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 1: Split the data into training and testing sets\n",
    "X = credit_data_balanced.drop('default', axis=1)\n",
    "y = credit_data_balanced['default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e69927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 2: Standardize the data\n",
    "scaler         = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 3: Hyperparameter optimization using GridSearchCV\n",
    "param_grid  = {'C':       list(np.linspace(0.0001, 10, 10)), #[0.0001, 0.001, 0.01, 0.1, 1, 10, 100], \n",
    "               'penalty': ['l1', 'l2', 'elasticnet'], \n",
    "               'solver':  ['saga', 'newton-cg', 'liblinear', 'lbfgs', 'sag']}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f06695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 4: Train the logistic regression model with the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "log_reg     = LogisticRegression(**best_params)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 5: Evaluate the model on the test set\n",
    "y_pred       = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 6: Display performance metrics\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params, '\\n')\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred), '\\n')\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred), '\\n')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 7: Plot the ROC curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc_score(y_test, y_pred_proba))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec830ce",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- **ROC Curve**: In the script above, we added a section to plot the ROC (Receiver Operating Characteristic) curve. The ROC curve is a graphical representation of the true positive rate against the false positive rate for the logistic regression model.\n",
    "  \n",
    "- **AUC Score**: The area under the ROC curve (AUC) score is also calculated and displayed in the legend of the plot. The AUC score gives us a single value summary of the performance of the model, where a score of 0.5 indicates a model with no discriminative power, and a score of 1.0 indicates a perfect model.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **True Positive Rate (Sensitivity)**: It is the ratio of the number of positive instances correctly predicted by the model to the total number of positive instances. It is given by the formula: TPR = TP / (TP + FN).\n",
    "  - **False Positive Rate (1 - Specificity)**: It is the ratio of the number of negative instances incorrectly predicted as positive by the model to the total number of negative instances. It is given by the formula: FPR = FP / (FP + TN).\n",
    "  - **ROC Curve**: The ROC curve is created by plotting the TPR against the FPR at various threshold settings. It gives us a sense of the trade-off between the true positive rate and false positive rate.\n",
    "  - **AUC**: The AUC gives us a single value metric to compare models. A higher AUC indicates a better performing model.\n",
    "\n",
    "- **Thresholds**: The ROC curve is created by varying the threshold used to classify instances as positive or negative. By analyzing the ROC curve, you can choose a threshold that gives a good balance between sensitivity and specificity, depending on the specific requirements of your problem.\n",
    "\n",
    "This script will now plot the ROC curve and display the AUC score in addition to the other performance metrics, giving you a more comprehensive view of the model's performance. Adjustments might be needed based on further analysis and insights derived from the EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb93bac",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea9277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ccacdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dba091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d95f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd15501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515a9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed45b947",
   "metadata": {},
   "source": [
    "### Conclusions <a class=\"anchor\" id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f5e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52942a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2947ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4382bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221983b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6409085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c43cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b2ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa55b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6cea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996d85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389faaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84754f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd659f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d428035",
   "metadata": {},
   "source": [
    "### Appendix <a class=\"anchor\" id=\"appendix\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e50e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75192fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e4c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "data = pd.read_excel(url, header=1)\n",
    "data.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "data.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X = data.drop('default', axis=1)\n",
    "y = data['default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "# Step 5: Define the hyperparameter space\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 300)),\n",
    "    'max_depth': hp.choice('max_depth', range(1, 20)),\n",
    "    'min_samples_split': hp.choice('min_samples_split', range(2, 20)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', range(2, 20)),\n",
    "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']),\n",
    "}\n",
    "\n",
    "# Step 6: Run hyperopt optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# Step 7: Train the Random Forest model with the best hyperparameters\n",
    "best_params = {\n",
    "    'n_estimators': best['n_estimators'] + 50,\n",
    "    'max_depth': best['max_depth'] + 1,\n",
    "    'min_samples_split': best['min_samples_split'] + 2,\n",
    "    'min_samples_leaf': best['min_samples_leaf'] + 1,\n",
    "    'max_features': ['auto', 'sqrt', 'log2'][best['max_features']],\n",
    "}\n",
    "rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 8: Evaluate the model on the test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "y_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Display performance metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 10: Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % roc_auc_score(y_test, y_pred_proba))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad60b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53495cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272a920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62be1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eaa8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b29b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cdfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8deb08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████████                                                                                                                                                                                             | 2/20 [07:58<1:03:39, 212.21s/trial, best loss: -0.8105]"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "data = pd.read_excel(url, header=1)\n",
    "data.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "data.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X = data.drop('default', axis=1)\n",
    "y = data['default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    model = SVC(**params, probability=True, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "# Step 5: Define the hyperparameter space\n",
    "space = {\n",
    "    'C': hp.loguniform('C', -4, 2),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
    "    'gamma': hp.choice('gamma', ['scale', 'auto']),\n",
    "    'degree': hp.choice('degree', [2, 3, 4, 5, 6]),\n",
    "}\n",
    "\n",
    "# Step 6: Run hyperopt optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "\n",
    "# Step 7: Train the SVM model with the best hyperparameters\n",
    "best_params = {\n",
    "    'C': best['C'],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'][best['kernel']],\n",
    "    'gamma': ['scale', 'auto'][best['gamma']],\n",
    "    'degree': best['degree'] + 2,\n",
    "}\n",
    "svm_model = SVC(**best_params, probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 8: Evaluate the model on the test set\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "y_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Display performance metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 10: Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='SVM (area = %0.2f)' % roc_auc_score(y_test, y_pred_proba))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92652618",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- **Support Vector Machines (SVM)**: In step 7, we used the `SVC` class from `scikit-learn` to build and train the SVM model.\n",
    "- **Hyperparameter Space**: In step 5, we defined a space of hyperparameters to search, including the regularization parameter `C`, the kernel type, the kernel coefficient `gamma`, and the degree of the polynomial kernel function.\n",
    "- **Objective Function**: In step 4, we defined an objective function that takes a set of hyperparameters, trains an SVM model, and returns the negative accuracy as the loss to be minimized by `hyperopt`.\n",
    "- **Bayesian Optimization**: In step 6, we used `hyperopt` to perform Bayesian optimization, using the Tree-structured Parzen Estimator (TPE) algorithm to find the best hyperparameters over 50 evaluations.\n",
    "- **Best Hyperparameters**: In step 7, we extracted the best hyperparameters from the optimization results and trained the SVM model using those hyperparameters.\n",
    "\n",
    "This script now uses an SVM model with Bayesian optimization for hyperparameter tuning, aiming to find the best hyperparameters in a more efficient manner compared to grid search. Adjustments might be needed based on further analysis and insights derived from the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575c5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf87ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e5d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbdf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f3f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e64b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93ad35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9757a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ca90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063d834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d346b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407d020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af549f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bf549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67ed7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465308f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd401871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e83e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb3176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f750b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99accec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddec5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb04962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fea72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cd19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719f957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b26bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fd3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a35a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728eb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb4d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a956c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ef36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression con ottimizzazzione bayes\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "data = pd.read_excel(url, header=1)\n",
    "data.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "data.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X = data.drop('default', axis=1)\n",
    "y = data['default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "# Step 5: Define the hyperparameter space\n",
    "space = {\n",
    "    'C': hp.loguniform('C', -4, 2),\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "    'solver': hp.choice('solver', ['liblinear'])\n",
    "}\n",
    "\n",
    "# Step 6: Run hyperopt optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# Step 7: Train the logistic regression model with the best hyperparameters\n",
    "best_params = {\n",
    "    'C': best['C'],\n",
    "    'penalty': ['l1', 'l2'][best['penalty']],\n",
    "    'solver': 'liblinear'\n",
    "}\n",
    "log_reg = LogisticRegression(**best_params)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 8: Evaluate the model on the test set\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Display performance metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 10: Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc_score(y_test, y_pred_proba))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdc4c8",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- **Hyperparameter Space**: In step 5, we defined the hyperparameter space using `hyperopt`'s `hp` module. We defined a log-uniform distribution for `C` to explore a wide range of values on a logarithmic scale. We also defined the choices for the `penalty` and `solver` parameters.\n",
    "- **Objective Function**: In step 4, we defined an objective function that takes a set of hyperparameters as input, trains a logistic regression model using those hyperparameters, and returns the negative accuracy as the loss. `hyperopt` minimizes the loss, so we return the negative accuracy to ensure that `hyperopt` is maximizing the accuracy.\n",
    "- **Bayesian Optimization**: In step 6, we used `hyperopt`'s `fmin` function to perform Bayesian optimization. We used the Tree-structured Parzen Estimator (TPE) as the optimization algorithm and performed 50 evaluations to find the best hyperparameters.\n",
    "- **Best Hyperparameters**: In step 7, we extracted the best hyperparameters from the optimization results and trained the logistic regression model using those hyperparameters.\n",
    "\n",
    "This script now uses Bayesian optimization for hyperparameter tuning, which can potentially find better hyperparameters in fewer iterations compared to grid search. Adjustments might be needed based on further analysis and insights derived from the EDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
